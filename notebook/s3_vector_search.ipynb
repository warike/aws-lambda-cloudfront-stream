{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d9527bc3",
            "metadata": {},
            "source": [
                "# S3 Vector Search Notebook\n",
                "\n",
                "This notebook demonstrates how to interact with **S3 Vectors** by:\n",
                "1. Connecting to the S3 Vector Index using the `s3vectors` boto3 client.\n",
                "2. Generating embeddings for a query using the Titan v2 model.\n",
                "3. Performing a similarity search directly against the S3 Vector Index.\n",
                "4. Generating an answer using an LLM (Llama 3) based on the retrieved context, with latency reporting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "id": "d34a4a4f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Libraries imported and environment variables loaded.\n"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries\n",
                "import boto3\n",
                "import os\n",
                "import json\n",
                "import secrets\n",
                "import string\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv(override=True)\n",
                "\n",
                "print(\"‚úÖ Libraries imported and environment variables loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "id": "70666761",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "AWS Region: us-east-1\n",
                        "AWS Profile: warike-development\n",
                        "Bedrock Embedding Model: amazon.titan-embed-text-v2:0\n",
                        "Bedrock Generation Model: us.meta.llama3-1-8b-instruct-v1:0\n",
                        "S3 Vector Bucket: bucket-vector-s3-lambda\n",
                        "S3 Vector Index: documents\n",
                        "‚úÖ S3 Vectors client initialized.\n"
                    ]
                }
            ],
            "source": [
                "# Configuration\n",
                "aws_region = os.getenv(\"AWS_REGION\", \"us-west-2\")\n",
                "aws_profile = os.getenv(\"AWS_PROFILE\", \"default\")\n",
                "bedrock_embedding_model_id = os.getenv(\"BEDROCK_EMBEDDING_MODEL_ID\", \"amazon.titan-embed-text-v2:0\")\n",
                "bedrock_model_id = os.getenv(\"BEDROCK_MODEL_ID\", \"meta.llama3-1-8b-instruct-v1:0\")\n",
                "s3_vector_bucket_name = os.getenv(\"S3_VECTOR_BUCKET_NAME\")\n",
                "s3_vector_index_name = os.getenv(\"S3_VECTOR_INDEX_NAME\")\n",
                "\n",
                "print(f\"AWS Region: {aws_region}\")\n",
                "print(f\"AWS Profile: {aws_profile}\")\n",
                "print(f\"Bedrock Embedding Model: {bedrock_embedding_model_id}\")\n",
                "print(f\"Bedrock Generation Model: {bedrock_model_id}\")\n",
                "print(f\"S3 Vector Bucket: {s3_vector_bucket_name}\")\n",
                "print(f\"S3 Vector Index: {s3_vector_index_name}\")\n",
                "\n",
                "# Setup AWS Session\n",
                "session = boto3.Session(profile_name=aws_profile, region_name=aws_region)\n",
                "bedrock_client = session.client(\"bedrock-runtime\")\n",
                "\n",
                "# Initialize S3 Vectors Client\n",
                "try:\n",
                "    s3_vectors_client = session.client(\"s3vectors\")\n",
                "    print(\"‚úÖ S3 Vectors client initialized.\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Failed to initialize 's3vectors' client. Ensure your boto3 version supports it. Error: {e}\")\n",
                "    s3_vectors_client = None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b11baec",
            "metadata": {},
            "source": [
                "## Step 1: Generate Embeddings\n",
                "Use the Titan v2 model to generate embeddings for a sample query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "id": "e5408501",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Generated embedding for query: 'Explain how EventBridge works in LLM Workflow context'\n",
                        "Embedding dimension: 1024\n"
                    ]
                }
            ],
            "source": [
                "def get_embedding(text, model_id=bedrock_embedding_model_id):\n",
                "    try:\n",
                "        body = json.dumps({\n",
                "            \"inputText\": text,\n",
                "            # Optional: \"dimensions\": 1024, \"normalize\": True\n",
                "        })\n",
                "        \n",
                "        response = bedrock_client.invoke_model(\n",
                "            body=body,\n",
                "            modelId=model_id,\n",
                "            accept=\"application/json\",\n",
                "            contentType=\"application/json\"\n",
                "        )\n",
                "        \n",
                "        response_body = json.loads(response.get(\"body\").read())\n",
                "        embedding = response_body.get(\"embedding\")\n",
                "        return embedding\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error generating embedding: {e}\")\n",
                "        return None\n",
                "\n",
                "# Test Query\n",
                "query_text = \"Explain how EventBridge works in LLM Workflow context\"\n",
                "query_embedding = get_embedding(query_text)\n",
                "\n",
                "if query_embedding:\n",
                "    print(f\"‚úÖ Generated embedding for query: '{query_text}'\")\n",
                "    print(f\"Embedding dimension: {len(query_embedding)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d152829",
            "metadata": {},
            "source": [
                "## Step 2: Similarity Search with Latency Reporting\n",
                "Perform a search against the S3 Vector Index and measure latency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "id": "b5616590",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Searching in Vector Bucket: bucket-vector-s3-lambda, Index: documents...\n",
                        "‚úÖ Search Results found in 0.8367 seconds.\n"
                    ]
                }
            ],
            "source": [
                "def search_vector_store(query_vector, bucket_name, index_name):\n",
                "    \"\"\"\n",
                "    Performs a similarity search using the s3vectors client and returns results with latency.\n",
                "    \"\"\"\n",
                "    if not s3_vectors_client:\n",
                "        print(\"‚ùå S3 Vectors client is not initialized.\")\n",
                "        return [], 0\n",
                "\n",
                "    if not bucket_name or not index_name:\n",
                "        print(\"‚ö†Ô∏è S3_VECTOR_BUCKET_NAME or S3_VECTOR_INDEX_NAME is not set. Skipping search.\")\n",
                "        return [], 0\n",
                "\n",
                "    print(f\"üîç Searching in Vector Bucket: {bucket_name}, Index: {index_name}...\")\n",
                "    \n",
                "    start_time = time.time()\n",
                "    try:\n",
                "        response = s3_vectors_client.query_vectors(\n",
                "            vectorBucketName=bucket_name,\n",
                "            indexName=index_name,\n",
                "            queryVector={\"float32\": query_vector},\n",
                "            topK=5,\n",
                "            returnDistance=True,\n",
                "            returnMetadata=True\n",
                "        )\n",
                "        end_time = time.time()\n",
                "        latency = end_time - start_time\n",
                "        \n",
                "        vectors = response.get('vectors', [])\n",
                "        print(f\"‚úÖ Search Results found in {latency:.4f} seconds.\")      \n",
                "        results = []\n",
                "        for result in vectors:\n",
                "            metadata = result.get('metadata', {})\n",
                "            chunk_text = metadata.get('chunk', '')\n",
                "            results.append(chunk_text)\n",
                "            \n",
                "        return results, latency\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error querying S3 Vectors: {e}\")\n",
                "        return [], 0\n",
                "\n",
                "# Execute Search\n",
                "retrieved_contexts, search_latency = search_vector_store(query_embedding, s3_vector_bucket_name, s3_vector_index_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "generate_answer_md",
            "metadata": {},
            "source": [
                "## Step 3: Generate Answer with Latency Reporting\n",
                "Use the retrieved context to generate an answer using the Bedrock model, ensuring no hallucinations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "id": "generate_answer_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ü§ñ Generating answer using us.meta.llama3-1-8b-instruct-v1:0...\n",
                        "‚úÖ Answer Generated in 1.8723 seconds:\n",
                        "\n",
                        "\n",
                        "\n",
                        "In the context of LLM (Large Language Model) workflow, Amazon EventBridge is not directly involved. Instead, the LLM classifies and interprets the user's intent through natural language, which is a different approach from traditional EventBridge rules-based routing.\n",
                        "\n",
                        "However, the provided context does mention that traditional dynamic dispatch uses Amazon EventBridge rules for routing based on structured event attributes. This implies that EventBridge is used in a separate workflow, not in conjunction with LLM-based routing.\n",
                        "\n",
                        "--- Performance Report ---\n",
                        "S3 Vector Search Latency: 0.8367s\n",
                        "LLM Generation Latency:   1.8723s\n",
                        "Total Latency:            2.7090s\n"
                    ]
                }
            ],
            "source": [
                "def generate_answer(query, contexts, model_id=bedrock_model_id):\n",
                "    if not contexts:\n",
                "        print(\"‚ö†Ô∏è No contexts retrieved. Skipping generation.\")\n",
                "        return\n",
                "\n",
                "    # Construct Context String\n",
                "    context_str = \"\\n\\n\".join(contexts)\n",
                "    \n",
                "    # Construct Prompt (Llama 3 Instruct Format)\n",
                "    prompt_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "You are a helpful assistant. Answer the user's question using ONLY the context provided below. \n",
                "If the answer is not in the context, say \"I don't know\" or \"The provided context does not contain the answer.\"\n",
                "Do not hallucinate or use outside knowledge.\n",
                "\n",
                "Context:\n",
                "{context_str}\n",
                "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "{query}\n",
                "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
                "\n",
                "    body = json.dumps({\n",
                "        \"prompt\": prompt_template,\n",
                "        \"temperature\": 0.1, # Low temperature to reduce hallucination\n",
                "        \"top_p\": 0.9\n",
                "    })\n",
                "\n",
                "    print(f\"ü§ñ Generating answer using {model_id}...\")\n",
                "    \n",
                "    start_time = time.time()\n",
                "    try:\n",
                "        response = bedrock_client.invoke_model(\n",
                "            body=body,\n",
                "            modelId=model_id,\n",
                "            accept=\"application/json\",\n",
                "            contentType=\"application/json\"\n",
                "        )\n",
                "        end_time = time.time()\n",
                "        latency = end_time - start_time\n",
                "        \n",
                "        response_body = json.loads(response.get(\"body\").read())\n",
                "        generation = response_body.get('generation')\n",
                "        \n",
                "        print(f\"‚úÖ Answer Generated in {latency:.4f} seconds:\\n\")\n",
                "        print(generation)\n",
                "        return latency\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error invoking Bedrock: {e}\")\n",
                "        return 0\n",
                "\n",
                "# Execute Generation\n",
                "generation_latency = generate_answer(query_text, retrieved_contexts)\n",
                "\n",
                "print(\"\\n--- Performance Report ---\")\n",
                "print(f\"S3 Vector Search Latency: {search_latency:.4f}s\")\n",
                "print(f\"LLM Generation Latency:   {generation_latency:.4f}s\")\n",
                "print(f\"Total Latency:            {search_latency + generation_latency:.4f}s\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
